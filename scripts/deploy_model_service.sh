#!/bin/bash

set -ex

CLUSTER_NAME="${CLUSTER_NAME:-rhoai}"
OPENSHIFT_CLIENT="${OPENSHIFT_CLIENT:-$(which oc)}"

export KUBECONFIG="../clusters/${CLUSTER_NAME}/auth/kubeconfig"

# Checking the cluster health
if ! ${OPENSHIFT_CLIENT} get clusterversion version -o jsonpath='{range .status.conditions[*]}{.type}={.status} {end}' | grep -E -q 'Available=True.*Progressing=False|Progressing=False.*Available=True'; then
  echo "Cluster is DEGRADED or UPDATING (Check 'oc get clusterversion')"
  exit 1
fi

echo "Setting a rhaiis/vllm-cuda-rhel9 vLLM runtime to deploy the RedHatAI/Llama-3.2-1B-Instruct-FP8 model for an Inference Chat Endpoint"

${OPENSHIFT_CLIENT} create namespace vllm-llama || true

# Persistent Volume Claim for HuggingFace cache
echo "Creating a Persistent Volume Claim for HuggingFace cache"

cat << EOF | ${OPENSHIFT_CLIENT} apply -f -
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: huggingface-cache-pvc
  namespace: vllm-llama
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 40Gi
EOF

# Deployment for the vLLM application
echo "Deploying the vLLM application"

cat << EOF | ${OPENSHIFT_CLIENT} apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama-deployment
  namespace: vllm-llama
  labels:
    app: vllm-llama
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm-llama
  template:
    metadata:
      labels:
        app: vllm-llama
    spec:
      containers:
        - name: vllm-container
          image: registry.redhat.io/rhaiis/vllm-cuda-rhel9
          args:
            - "--model=RedHatAI/Llama-3.2-1B-Instruct-FP8"
            - "--tensor-parallel-size=1"
            - "--host=0.0.0.0"
            - "--port=8000"
          env:
            - name: HF_HUB_OFFLINE
              value: "0"
            - name: VLLM_NO_USAGE_STATS
              value: "1"
          ports:
            - containerPort: 8000
              name: http
              protocol: TCP
          # Adding best-practice security settings.
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - "ALL"
          resources:
            requests:
              cpu: "4"
              memory: "8Gi"
            limits:
              # This requests one NVIDIA GPU. Requires the NVIDIA GPU Operator.
              nvidia.com/gpu: '1'
              cpu: "8"
              memory: "16Gi"
          volumeMounts:
            # Mount the persistent cache volume
            - name: huggingface-cache
              mountPath: /root/.cache/huggingface
            # Mount the shared memory volume
            - name: dshm
              mountPath: /dev/shm
      volumes:
        - name: huggingface-cache
          persistentVolumeClaim:
            claimName: huggingface-cache-pvc
        # Define the 4Gi shared memory volume
        - name: dshm
          emptyDir:
            medium: Memory
            sizeLimit: 4Gi
EOF

echo "Waiting for the vllm-llama deployment to be available"
${OPENSHIFT_CLIENT} wait --for=condition=Available --timeout=30m deployment/vllm-llama-deployment -n vllm-llama

VLLM_POD_NAME=$(${OPENSHIFT_CLIENT} get pods -n vllm-llama -o jsonpath='{.items[0].metadata.name}')

timeout 600s bash -c "while ! (${OPENSHIFT_CLIENT} logs pod/${VLLM_POD_NAME} -n vllm-llama | grep 'Application startup complete'); do sleep 10; done"

# Service to expose the Deployment internally
echo "Creating a Service to expose the Deployment internally"

cat << EOF | ${OPENSHIFT_CLIENT} apply -f -
apiVersion: v1
kind: Service
metadata:
  name: vllm-llama-service
  namespace: vllm-llama
  labels:
    app: vllm-llama
spec:
  selector:
    app: vllm-llama
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
  type: ClusterIP
EOF

# Route to expose the Service externally
echo "Creating a Route to expose the Service externally"

cat << EOF | ${OPENSHIFT_CLIENT} apply -f -
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: vllm-llama-route
  namespace: vllm-llama
  labels:
    app: vllm-llama
spec:
  to:
    kind: Service
    name: vllm-llama-service
  port:
    targetPort: 8000
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
EOF

# Getting the Floating IP for the worker node that hosts the Openshift Ingress Router Default

ROUTER_WORKER=$(${OPENSHIFT_CLIENT} get pods -n openshift-ingress -o jsonpath='{.items[0].spec.nodeName}')

ROUTER_FLOATINGIP=$(openstack server show "${ROUTER_WORKER}" -f value -c addresses | grep -oP '192\.168\.122\.\d{1,3}')

INFERENCE_ENDPOINT=$(${OPENSHIFT_CLIENT} get route -n vllm-llama -o jsonpath='{.items[0].spec.host}')

hosts="# Generated by rhos-vaf for Model Service $CLUSTER_NAME - Do not edit
$ROUTER_FLOATINGIP ${INFERENCE_ENDPOINT}
# End of rhos-vaf $CLUSTER_NAME inference endpoints"

old_hosts=$(awk "/# Generated by rhos-vaf for Model Service $CLUSTER_NAME - Do not edit/,/# End of rhos-vaf $CLUSTER_NAME inference endpoints/" /etc/hosts)

if [ "${hosts}" != "${old_hosts}" ]; then
    echo Updating hosts file
    sudo sed -i "/# Generated by rhos-vaf for Model Service $CLUSTER_NAME - Do not edit/,/# End of rhos-vaf $CLUSTER_NAME inference endpoints/d" /etc/hosts
    echo "$hosts" | sudo tee -a /etc/hosts
fi

# Access to the  Model Service - Inference Chat
echo "Model Service is ready!!! Now you can chat and request metrics with the following inference endpoint https://${INFERENCE_ENDPOINT}"
